# Reddit Grabber â€” Functional Specification

*VersionÂ 0.9 Â· MayÂ 28Â 2025*

---

## 1Â Â·Â Purpose

The **RedditÂ Grabber** is a crossâ€‘platform utility that downloads every image **and** video referenced in a Reddit submission or a filtered slice of a subreddit feed.  It is designed for:

* **Archival** â€” keep local copies of highâ€‘quality media before links rot or hosts disappear.
* **Curation** â€” enable hobbyists to bulkâ€‘collect themed media (e.g., wallpapers, references, meme datasets).
* **Automation** â€” provide a composable CLI & API for scripts, cron jobs, or GUI wrappers.

---

## 2Â Â·Â Scope

| âœ…Â Included                                                                                                        | âŒÂ Excluded                                   |
| ----------------------------------------------------------------------------------------------------------------- | -------------------------------------------- |
| Commandâ€‘line interface (CLI)                                                                                      | Mobile apps                                  |
| Textual TUI dashboard (Terminal UI)                                                                               | FullÂ Electron desktop app (PhaseÂ 2)          |
| Optional PySide6 / Qt desktop wrapper (PhaseÂ 1.5)                                                                 | Writing a cloud backend                      |
| Image + video retrieval from: Reddit CDN (`i.redd.it`, `v.redd.it`), Imgur, Redgifs, Gfycat, generic direct links | Support for private / loginâ€‘gated subreddits |
| Deâ€‘duplication & manifest logging                                                                                 | Advanced ML tagging                          |
| SQLite cache for resumable sync                                                                                   | Torrent seeding                              |
| MIT openâ€‘source release                                                                                           | Commercial license management                |

---

## 3Â Â·Â Glossary

| Term             | Meaning                                                                                  |
| ---------------- | ---------------------------------------------------------------------------------------- |
| **Submission**   | A single Reddit post (image, link, text, gallery, video).                                |
| **SearchÂ slice** | A set of submissions returned by a combination of subreddit + filters.                   |
| **Grab**         | The act of downloading every resolvable media resource linked to a submission.           |
| **Sync**         | Iteratively grab all submissions in a search slice, skipping duplicates using the cache. |
| **Hash**         | SHAâ€‘1 of a downloaded file used for deâ€‘duplication across runs.                          |
| **Manifest**     | `manifest.json` written perâ€‘submission containing metadata & resolved media URLs.        |

---

## 4Â Â·Â System Context

```mermaid
flowchart LR
    User -->|CLI / GUI| Grabber[Redditâ€‘Grabber]
    Grabber -->|OAuth| RedditAPI((Reddit REST API))
    Grabber -->|HTTP| MediaHosts((i.redd.it / v.redd.it / imgur / â€¦))
    Grabber --> SQLiteCache[(grabber.db)]
    Grabber --> LocalFS[(Downloads folder)]
```

*No server component is required; all calls originate from the endâ€‘user machine.*

---

## 5Â Â·Â Functional Requirements

### 5.1Â CLI Commands

| Command                         | Synopsis                                                                   | Description |
| ------------------------------- | -------------------------------------------------------------------------- | ----------- |
| `grabber grab <submissionâ€‘url>` | Download all media from one post.                                          |             |
| `grabber sync <subreddit>`      | Traverse a subreddit using the **search layer** & download matching media. |             |
| `grabber dbâ€‘vacuum`             | Clean and optimise the SQLite cache.                                       |             |
| `grabber version`               | Print semantic version & build info.                                       |             |

#### 5.1.1Â GlobalÂ Flags

* `â€‘â€‘output <dir>`Â â€” custom destination root (default `./downloads`).
* `â€‘â€‘config <path>`Â â€” load extra defaults from a TOML/YAML config file.
* `â€‘â€‘quiet / â€‘q`Â â€” suppress normal log lines (errors only).
* `â€‘â€‘ui {none,textual,pyside}`Â â€” force chosen UI wrapper.

### 5.2Â Search Filters (`sync` only)

| Flag                          | Description                                                 | Example              |
| ----------------------------- | ----------------------------------------------------------- | -------------------- |
| `â€‘â€‘query <str>`               | Keyword search in *title* (ANDâ€‘joined).                     | `"corgi beach"`      |
| `â€‘â€‘flair <regex>`             | Include only posts whose flair matches regex.               | `"(?i)OC"`           |
| `â€‘â€‘since / â€‘â€‘until`           | ISOÂ date or duration (`3d`, `6h`).                          | `--since 2025â€‘05â€‘01` |
| `â€‘â€‘minâ€‘score <n>`             | Skip submissions below Reddit score.                        | `--min-score 100`    |
| `â€‘â€‘media {images,videos,all}` | Preâ€‘filter by `post_hint` & URL.                            |                      |
| `â€‘â€‘user <name>`               | Only posts by specific author(s).                           | multiple allowed     |
| `â€‘â€‘limit <n>`                 | Max submissions fetched this run.                           | default 100          |
| `â€‘â€‘allowâ€‘nsfw`                | Explicitly permit NSFW content.                             | optâ€‘in               |
| `â€‘â€‘pushshift`                 | Fall back to Pushshift API when Reddit search is throttled. | optional             |

### 5.3Â Download Behaviour

* Video downloading delegated to **ytâ€‘dlp** â€” handles DASH (v.redd.it), HLS, and external embeds.
* Images fetched via `requests` with 15Â s timeout, 3Â retries (exponential backâ€‘off).
* Output path pattern (configurable):
  `Â«OUTPUT_ROOTÂ»/{subreddit}/{id}/{safe_title}/`.
* SHAâ€‘1 computed after write.  If hash already exists in cache, file is discarded.

### 5.4Â Manifest JSON Schema (`v1`)

```jsonc
{
  "id": "abc123",
  "subreddit": "aww",
  "title": "Very smol corgiâ€¦",
  "author": "u/doge",
  "permalink": "/r/aww/comments/abc123/â€¦",
  "utc_timestamp": 1753123123,
  "score": 5321,
  "flair": "OC",
  "downloaded": [
    "corgi.jpg",
    "corgi_zoomies.mp4"
  ]
}
```

### 5.5Â SQLite Cache

```sql
CREATE TABLE files (
  hash       TEXT PRIMARY KEY,
  path       TEXT,
  first_seen INTEGER
);
CREATE TABLE posts (
  id         TEXT PRIMARY KEY,
  permalink  TEXT,
  downloaded INTEGER DEFAULT 0,
  last_check INTEGER
);
```

*Resumable sync* queries `posts.downloaded` to avoid reâ€‘processing.

### 5.6Â Configuration Sources (priority order)

1. **CLI flags**
2. `GRABBER_â€¦` environment variables
3. `~/.config/redditâ€‘grabber/config.toml`
4. Projectâ€‘local `grabber.toml`

### 5.7Â Authentication

* Uses **applicationâ€‘only OAuth** (Reddit â€œscriptâ€ app).
* Secrets are loaded from `.env` or envâ€‘vars & never logged.

### 5.8Â NSFW & Privacy

* NSFW posts are **skipped** unless `--allow-nsfw` is set.
* Filenames are sanitized; no user PII beyond Reddit username is stored.

---

## 6Â Â·Â Userâ€‘Interface Specifications

### 6.1Â TextualÂ TUI (default when terminal â‰¤Â 120Â cols)

```
â”Œ Redditâ€‘Grabber â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [Subreddit]  r/aww   [Query] corgi      â”‚
â”‚ [ğŸ”  Fetch]  Limit: 100  NSFW: Off      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ # | â†‘Score |  Title                     â”‚
â”‚ 1 | 5.3k   | Very smol leggedâ€¦         â”‚
â”‚ 2 | 2.1k   | [OC] beach zoomies (vid)  â”‚
â”‚Â·Â·Â·                                      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚
â”‚ â¬‡ Download queue                        â”‚
â”‚ âœ” corgi_zoomies.mp4  âœ“ 14Â MB            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

* Arrow keys / mouse select rows â†’ **Space** adds to queue.
* `D` toggles autoâ€‘download.
* Live Rich progress bars per file.

### 6.2Â PySide6 Desktop (optional)

* Menu bar: *FileÂ >Â NewÂ Sync*, *ToolsÂ >Â Vacuum Cache*.
* Central QTableView bound to `QAbstractTableModel` of search results.
* Dragâ€‘andâ€‘drop output folder chooser.

### 6.3Â Web Miniâ€‘Server (future)

* Flask + HTMX, served at `localhost:8000`.
* No auth; CORSÂ =Â `sameâ€‘origin`.

---

## 7Â Â·Â Nonâ€‘Functional Requirements

| Category          | Requirement                                                                                |
| ----------------- | ------------------------------------------------------------------------------------------ |
| **Performance**   | 20 concurrent downloads max, adaptive to host throttling.                                  |
| **Portability**   | Runs on PythonÂ â‰¥Â 3.9, Windows/macOS/Linux.                                                 |
| **Reliability**   | Safe resume after abrupt termination; database journaling on by default.                   |
| **Security**      | No secrets written to manifest or logs; follows Reddit API TOS.                            |
| **Accessibility** | TUI supports screenâ€‘reader friendly Rich markup; Qt UI adheres to WCAG AA colour contrast. |
| **License**       | MIT (OSIâ€‘approved).                                                                        |

---

## 8Â Â·Â Architecture

* **grabber.cli** â€” Typer CLI facade
* **grabber.search** â€” constructs `SearchParams`, yields `praw.models.Submission`
* **grabber.downloader** â€” idempotent file fetcher + ytâ€‘dlp wrapper
* **grabber.database** â€” thin SQLiteÂ DAL (SqliteDict or bare `sqlite3`)
* **grabber.ui** â€” pluggable frontâ€‘ends reusing service layer

All heavy I/O (download, DB writes) pushes to a `ThreadPoolExecutor` to keep UIs responsive.

---

## 9Â Â·Â Error Handling & Logging

| Level | Sink                    | Example                                  |
| ----- | ----------------------- | ---------------------------------------- |
| DEBUG | hidden unless `â€‘â€‘debug` | Full Reddit JSON payload                 |
| INFO  | stdout (Rich)           | â€œâœ“Â Saved 2 files toÂ â€¦/aww/abc123/â€       |
| WARN  | stderr                  | â€œRetrying (2/3) 503 Service Unavailableâ€ |
| ERROR | stderr & `grabber.log`  | Tracebacks with timestamp                |

---

## 10Â Â·Â Dependencies

* **prawÂ â‰¥Â 7.7** (or *asyncpraw* later)
* **ytâ€‘dlpÂ â‰¥Â 2025.04**
* **requestsÂ â‰¥Â 2.32**
* **typerÂ â‰¥Â 0.12** + **richÂ â‰¥Â 13**
* **textualÂ â‰¥Â 0.50** (TUI)
* **pythonâ€‘dotenvÂ â‰¥Â 1.0**
* **pytestÂ â‰¥Â 8** + **pytestâ€‘mock** (dev)

---

## 11Â Â·Â Testing & CI

1. **Unit** â€” mock Reddit API, assert search filters & fileâ€‘save logic.
2. **Integration** â€” nightly GitHubÂ Actions job against `r/redditdev` (public test sub).  Ensures no API regressions.
3. **Smoke** â€” run `grabber grab <knownÂ post>` inside docker image, checksum expected files.

---

## 12Â Â·Â Delivery & Milestones

| Milestone                  | Target | Contents                                                      |
| -------------------------- | ------ | ------------------------------------------------------------- |
| **M1** â€” Core CLI MVP      | DayÂ 5  | `grab` command, manifest JSON, env config.                    |
| **M2** â€” Search & Sync     | DayÂ 12 | `search` layer, `sync` with limit/date filters, SQLite cache. |
| **M3** â€” TextualÂ TUI       | DayÂ 18 | Live dashboard, queue management.                             |
| **M4** â€” Packaging         | DayÂ 22 | `pipx` install, Homebrew formula draft, Dockerfile.           |
| **M5** â€” DesktopÂ GUI Alpha | DayÂ 30 | PySide6 wrapper, installer skeleton.                          |

---

## 13Â Â·Â Future Enhancements

* Automatic EXIF stripping / metadata preservation toggle.
* Parallel S3 / GDrive uploader plugin.
* Duplicateâ€‘finder across previously archived subreddits.
* Hooks for Stable Diffusion tagging pipeline.
* WebSocket API for remote control.

---

*End of specification*
