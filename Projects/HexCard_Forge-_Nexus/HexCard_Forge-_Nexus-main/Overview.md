
# HexCard Forge Nexus – Project Overview & Prompt Walkthrough

## Project Purpose and Goals

**HexCard Forge Nexus** is a unified application (web & desktop) that combines three primary features into one seamless experience:

1. **Forge Characters:** Players fuse six “hex-cards” to create a new **character** with combined stats.
2. **Map Editor:** Players design or randomly generate hexagonal maps for gameplay.
3. **Card Game (Play Mode):** A turn-based strategy game where the forged characters battle on the hex map.

All three workflows share a **common data model** and UI theme, running in one codebase without separate modules. The goal is to allow users to switch between **Forge**, **Map**, and **Play** screens fluidly (no reloads). Every hex-card can be used anywhere (no mode-specific card types). The app targets both offline **desktop** use (via Tauri, a lightweight Rust-based desktop shell) and online **web** play, using the same code for both. Accessibility and extensibility are also considered (e.g. keyboard navigation, color-blind friendly design, AI-assisted features).

## High-Level Architecture

**Technology Stack:** HexCard Forge Nexus is built with a modern TypeScript stack. The front-end uses **React 18** with **Vite** for fast development and hot-reload. 3D and 2D rendering are handled by **Three.js** (for the 3D board and forge scenes) and **Pixi.js** (for 2D HUD and effects). The desktop version is packaged with **Tauri 2**, which wraps the web app in a tiny Rust-backed desktop application (for file system access and native integration). State management is handled by **Zustand** (a lightweight state store) for simplicity and ease of testing. Data validation uses **Zod** schemas to ensure that all game data (cards, characters, maps, etc.) conforms to the expected structure at runtime. Styling is done with **Tailwind CSS** (with CSS Modules) for a consistent theme, and testing is built into the workflow with **Vitest** (unit tests), **React Testing Library** (component tests), and **Playwright** (end-to-end tests). Continuous integration uses GitHub Actions to build and test across environments (Web, Windows, macOS, Linux) automatically.

**Monorepo Structure:** The project is organized as a **pnpm monorepo** with multiple packages and apps. The folder layout is as follows:

```plaintext
apps/
  web/             # React web application (browser UI)
  desktop/         # Tauri desktop shell (bundles the web app)
packages/
  ui/              # Reusable React UI components (shared across screens)
  engine/          # Core game logic (rules, turn system, forge algorithm)
  schema/          # Data models and validation (Zod schemas)
  utils/           # Utility modules (common helpers, etc.)
```

* **Apps:** The `apps/web` directory contains the browser-based React app (the main UI for Forge/Map/Play), while `apps/desktop` contains a Tauri project that loads the web app and provides native capabilities (like file system for save/load). Both share the same logic from the packages.
* **Packages:** `packages/schema` defines the **shared data models** using Zod (for example, definitions of a HexCard, Character, MapTile, etc.). `packages/engine` implements the **game mechanics** – e.g. the character fusion algorithm, turn-phase state machine, combat resolution, and any logic that must be consistent across Forge/Play modes. `packages/ui` holds common UI components (React components, styling, icons) reused in different screens. `packages/utils` would host general utilities. All packages are in TypeScript and are cross-used by the apps.

**Primary Data Models:** The core game entities are formalized in the schema package. For instance, a **HexCard** object represents a single card and includes an `id` (UUID), name, type (e.g. *unit, hero, spell, structure, relic*), six **edges** (icons around the hex card, such as attack/defense/element, etc.), optional tags, cost, and text. A simplified example of the HexCard schema in Zod is:

```ts
export const EdgeIcon = z.enum([
  "attack","defense","element","resource","skill","link"
]);
export const HexCard = z.object({
  id:    z.string().uuid(),
  name:  z.string(),
  type:  z.enum(["unit","hero","spell","structure","relic"]),
  edges: z.tuple([EdgeIcon, EdgeIcon, EdgeIcon, EdgeIcon, EdgeIcon, EdgeIcon]),
  tags:  z.string().array().default([]),
  cost:  z.number().int().nonnegative().optional(),
  text:  z.string().optional()
});
```



A **Character** (forged character) is defined as an object with its own `id`, a name, an array of six card IDs (the cards fused to create it), and generated attributes like a stats record and a list of abilities. There are also schemas for **MapTile** (hexagon grid tiles with terrain type and an optional occupant ID) and a top-level **SaveGame** structure containing the full game state (map layout, deck of all cards, list of forged characters, current turn/phase, etc.). Using Zod for these definitions means that whenever data is loaded or modified, it can be validated against the schema – any invalid data is rejected and triggers an error message (rather than causing hidden bugs).

**Module Design & Flow:** The application is divided into UI modules corresponding to the three main features, all operating on the shared state:

* **Navigation:** A persistent sidebar allows switching between **Forge**, **Map**, and **Play** views at any time. This is implemented via React Router, so the content panel swaps out the appropriate screen without unloading the app. Keyboard shortcuts (Ctrl/Cmd+1/2/3) provide quick navigation.
* **Forge (Character Fusion):** This screen presents a 3D **crucible** or table where players can drag and drop six cards onto slots. As cards are added, a live preview of the resulting character’s stats and abilities is shown. When the player clicks *Forge*, the character is finalized and added to the `forged` characters list in the global state. (An “AI Suggest” button is planned to offer card combo suggestions for forging, using either a simple heuristic or AI model).
* **Map Editor:** This mode lets players draw and edit a hexagonal grid map. There is a toolbar with tools like brush (paint terrain), fill, elevation adjustments, and placing props or spawn points. Users can also switch to a **Procedural Generation** tab to auto-generate a map by inputting a seed and adjusting sliders (for parameters like map size, water percentage, mountain ruggedness, etc.). The editor can export the map to a file (e.g. `.hexmap` JSON, which is a subset of the SaveGame format) or directly commit the edited map into the application state.
* **Play (Card Game Loop):** This is the actual game mode where players use their forged characters on the map in a turn-based strategy format. The board (hex grid) is rendered in 3D via Three.js, showing units on each hex. The game progresses in phases (upkeep, hero phase, action, combat, cleanup) as defined by a turn state machine in the `engine` module. Every action (moves, attacks, etc.) dispatched to the Zustand store is logged (allowing undo/redo and replay functionality). Combat and other mechanics are handled by pure functions in the engine for ease of unit testing.

All these modules share a **single global state store** (the Zustand store), which holds the deck of cards, current map, and characters, so that changes in one mode (e.g. forging a new character or editing the map) are immediately reflected if you switch to another mode. The data flow can be visualized as: *Forge → (updates) → Global Store ← (feeds) ← Map and Play*, with the ability to **save** or **load** the entire state to disk or browser storage as a JSON file at any time. The Save/Load operations also use Zod to validate imported data (if loading fails validation, the app will alert the user and keep a backup of the corrupt data rather than blindly accepting it).

**Testing and CI:** From the outset, the project is developed with testing in mind. Unit tests (with Vitest) will cover the game engine logic (e.g. card fusion math, turn order, reducers) aiming for ≥90% coverage. React components are tested with React Testing Library to verify UI behavior and state integration, and Playwright is used for end-to-end tests simulating user flows (for example: *create a map → forge a hero → start a match* should work without errors). Accessibility is checked via automated tools (axe-core with Playwright) to ensure compliance with WCAG 2.2 AA standards. Performance budgets (like load time under 2s, memory under 120MB) are to be verified with Lighthouse and Tauri’s bench tools. The project uses **pnpm** scripts to streamline development and testing (e.g. `pnpm dev:web` for running the web app locally, `pnpm dev:desktop` for the Tauri app, `pnpm test` for unit tests, and `pnpm e2e` for running the Playwright suite). Every commit/PR triggers GitHub Actions CI to run the tests on all major OSes to catch regressions early.

## Prompt Series Walkthrough (Code-Generation Workflow)

One unique aspect of this repository is the inclusion of a **test-driven development plan powered by AI code-generation prompts**. In the file `prompt_plan.md`, the development roadmap is broken into chunks, each accompanied by a GPT-4 prompt that a developer can feed into an LLM to generate the required code and tests for that chunk. The idea is to use the LLM as a pair-programmer: each prompt describes a set of tasks and the tests that must pass, and the LLM returns a patch (diff) implementing those tasks. The developer then runs the tests; only if they all pass (“green”) do they proceed to the next prompt. This enforces an iterative, test-driven workflow (red-green-refactor cycles) guided by the prompts.

Below is a walkthrough of the **key prompts** included in the repository, along with their intent, usage, and expected outputs. Each prompt is numbered in sequence and corresponds to a specific development chunk:

1. **Prompt #1 – Monorepo Setup (C0-1):** *Goal:* initialize the project structure. This prompt instructs the LLM to create the baseline **pnpm monorepo** with the directories and configs for `apps/web`, `apps/desktop`, and `packages/schema`. The tasks include running `pnpm init` with workspace settings and adding a minimal React+Vite app in `apps/web` (with a "Hello Forge" placeholder). It also mentions creating a baseline `.gitignore` and `README.md`. The **TDD criteria** require writing a simple React component and test: for example, creating `App.test.tsx` that expects an `<h1>Hello Forge</h1>` to be rendered, and then implementing the `App.tsx` component to make that test pass. After feeding this prompt to the LLM, the expected output would be a patch diff containing new files (package.json, pnpm workspace config, a React app scaffold with an App component and test, etc.). The developer would apply this patch, then run `pnpm install` and `pnpm test` – at this point, the test for the `<h1>Hello Forge</h1>` heading should pass, confirming the monorepo and app skeleton are set up correctly. (In practice, one would see a test runner output indicating a passed test for the App component.)

2. **Prompt #2 – Lint & Format Hooks (C0-2):** *Goal:* set up code linting and formatting enforcement. This prompt assumes the base project is in place and extends it by adding **ESLint and Prettier** configuration files. It also sets up **husky** and **lint-staged** for pre-commit hooks, so that code is automatically linted/formatted on each commit. The TDD approach here is clever: it suggests introducing a deliberately misformatted file (`apps/web/src/bad.ts`) and then adding a test (or script) that runs `pnpm run lint` and expects it to exit with code 0 after auto-fixing the issues. In other words, the prompt has the LLM configure the tools such that running the linter will fix problems. The output from the LLM should be a diff adding config files (`.eslintrc.json`, `.prettierrc`, etc.), updating package scripts (like `lint` script), and adding the husky hook. The developer would then run the linter to ensure it fixes `bad.ts` and that the test confirming a clean lint pass succeeds. Once this is green (meaning the lint tools are properly integrated), the project has enforced coding standards.

3. **Prompt #3 – Zod Schema & Tests (C1-1):** *Goal:* implement the **shared data models**. This prompt directs the LLM to create the `packages/schema/src/index.ts` file containing Zod schema definitions for the game’s data according to the specification (version 0.9). The test criteria given are two JSON fixtures: a `validCard.json` that should parse without errors, and an `invalidCard.json` (e.g. missing a required field like an `id`) that should cause the Zod schema to throw an error. The LLM, informed by the spec, would output code defining Zod schemas for HexCard, Character, MapTile, etc., and also produce corresponding Vitest unit tests for parsing those fixtures. For example, the HexCard schema would be defined as shown earlier (with fields for `id`, `name`, `type`, `edges`, etc.), and the test might do something like:

   ```ts
   import { HexCard } from "packages/schema";
   import validCard from "tests/__fixtures__/validCard.json";
   import invalidCard from "tests/__fixtures__/invalidCard.json";
   test("valid card JSON should parse", () => {
     expect(() => HexCard.parse(validCard)).not.toThrow();
   });
   test("invalid card JSON should throw", () => {
     expect(() => HexCard.parse(invalidCard)).toThrow();
   });
   ```

   After applying the LLM's patch, running `pnpm test` should show these tests passing (meaning the Zod models are correctly implemented to accept valid data and reject invalid data). This step establishes a **type-safe data layer** for the game.

4. **Prompt #4 – Zustand Store w/ Persistence (C1-2):** *Goal:* set up the **global state store** for the app. The prompt instructs creating a store (likely in `packages/engine/src/store.ts`) that uses Zustand to hold the application state, exposing a hook `useForgeStore` for components to access it. It also requires adding **state persistence** – in the web app, the store should persist to `localStorage` (so data isn’t lost on refresh), while in tests this persistence should be mocked or disabled. The TDD requirements include writing tests such as: after calling an action like `addCard(card)` on the store, a selector should return that card in the state, and another test to verify that the persistence middleware serializes the state to a JSON string (ensuring the save mechanism works). The LLM’s output would add the Zustand store implementation with actions (e.g., `addCard`, `removeCard`, etc.), and likely integration of `zustand/middleware` for persistence. Once integrated, running the unit tests should confirm that adding data to the store works and that the state is saved to storage. This global store is crucial for enabling the Forge, Map, and Play modules to share data in real-time.

5. **(Subsequent Prompts for Remaining Chunks):** The repository plan continues with similar prompts for each development **chunk** from C1-3 up to C8-3 (though these are summarized in the document). In each, the pattern is: define the tasks to implement a feature, and define tests to validate it. For example, prompts in Milestone M-1 would continue with implementing a Save/Load helper (C1-3, C1-4), including writing failing tests and then making them pass (likely ensuring that game state can be saved to a file and loaded back). Milestone M-2’s prompts would cover the **Forge MVP** – creating the Forge UI skeleton (possibly a 3D scene with drag-and-drop), implementing the card **fusion algorithm** that computes a character’s stats from six cards, and writing tests for that math. Milestone M-3 prompts would handle the **Map Editor MVP** – e.g. a component to render a hex grid, tools for terrain painting (with reducer logic tested), and file import/export functions for maps. Milestone M-4 prompts focus on the **Card Game loop** – implementing a pure turn state machine (with phases like upkeep, action, etc.), a combat resolution function for when two cards battle, and integrating these with a board UI, plus an end-to-end test of a minimal match. Milestone M-5 covers **cross-module integration** – e.g. a prompt for adding a sidebar navigation (linking Forge/Map/Play screens), another for implementing an **undo/redo** system (so state changes can be reverted), and a test ensuring that the state stays in sync across modules. Milestone M-6 adds **Procedural Generation and AI hints** – prompts here would ask the LLM to write a map generation function (with randomness controllable by a seed) and an AI suggestion function that gives card combos for forging, along with their respective tests. Milestone M-7 deals with **Desktop build & accessibility** – for example, creating the Tauri integration (IPC bridges between the Rust back-end and the front-end, with tests in Rust or TypeScript), integrating a native file dialog for save/load on desktop, and adding an automated accessibility audit (axe-core) in CI. Milestone M-8 prompts would address final **polish and release prep** – such as a balancing script to tweak game parameters (with snapshot tests to ensure consistency), adding particle effects or animations, and writing a release build script that packages the app and perhaps generates a checksum for the build. Each prompt in these sequences is run in order, ensuring each feature is built and tested before moving on.

6. **Final Integration Prompt – “Smoke” End-to-End Test:** After all features are implemented through the earlier prompts, the repository includes a **Final Wiring Prompt** that ties everything together. This last prompt instructs the LLM to wire up all modules and ensure the entire application works as a whole. The tasks outline a user flow: add a sidebar with icons for Map/Forge/Play, then simulate using the app – in the Forge tab, drop six mock cards to create a character; then switch to the Map tab and place that character on the grid; then start the Play tab and verify the turn state starts at the “hero” phase. The prompt provides an end-to-end test (likely a Playwright test `e2e/smoke.spec.ts`) that goes through these steps: visiting the app, forging a hero, placing the hero on a map, and asserting that the UI shows the expected game phase. The LLM’s output for this final prompt would be a comprehensive diff ensuring that any remaining wiring or fixes are done so that all unit tests and the smoke test pass. After applying it, the developer would run the full test suite and see everything green. At this point, the HexCard Forge Nexus MVP would be functionally complete and validated by automated tests.

**How to Invoke and Use the Prompts:** All the prompts are designed to be fed into a coding-capable Large Language Model (such as OpenAI’s GPT-4 with code interpreter or GitHub Copilot in CLI mode). The repository suggests a workflow where you copy Prompt #1 into the LLM and execute the returned code patch, then run tests; proceed to the next prompt only when tests pass. Each prompt is self-contained, meaning it provides enough context (file paths, required behaviors) so that the LLM can generate code that fits into the existing project. This approach effectively automates the TDD (Test-Driven Development) cycle: the prompt gives the “red” failing test upfront, the LLM produces code to make it “green”, and the developer verifies by running the tests. By following the prompts in order, a developer can gradually build out the entire application. In summary, the prompt series in the repository serves as a guided, step-by-step **implementation plan**, ensuring that at each step the code is correct and the final outcome meets the specification.

Overall, HexCard Forge Nexus is a well-thought-out project that marries a detailed specification with an AI-assisted development workflow. The repository’s structure and documentation highlight the project’s purpose (a unified card-forging strategy game), the high-level architecture (React/Three.js front-end with a shared state and data model, packaged for web and desktop), and a clear breakdown of tasks. The included GPT-4 prompts demonstrate how each feature can be developed in a test-driven manner, which not only helps in writing correct code but also serves as up-to-date documentation of the codebase’s intent. This makes the project relatively easy to onboard new developers – as noted in the spec, one can simply install dependencies with `pnpm i` and run `pnpm dev:web` or `pnpm dev:desktop` to start developing, with tests and prompts to guide the way. The combination of **comprehensive specs** and **iterative prompts** ensures that the final application will meet its goals and maintain high code quality from start to finish.

**Sources:** The information above is synthesized from the project’s specification document, development plan and prompt list, and to-do checklist in the HexCard\_Forge-\_Nexus repository.
